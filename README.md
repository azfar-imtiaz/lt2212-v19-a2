# LT2212 V19 Assignment 2

From Asad Sayeed's statistical NLP course at the University of Gothenburg.

My name: Muhammad Azfar Imtiaz

## Additional instructions
There are no additional instructions for running the code, it uses the same command-line arguments specified in the assignment instructions. However, there are some things that I would like to mention:

- The file `gendoc.py` outputs the results to a .csv file, so it expects the output file name specified to be of .csv format
- The file `simdoc.py` expects the input file specified in the command line arguments to be a .csv file
- The text preprocessing I have applied on the documents is a little extensive, and here is how it goes:
	- First of all, the text is converted to lowercase.
	- Next, stopwords are removed from the text using NLTK's English stopwords.
	- I noticed that in the documents, all the punctuation marks were prepended and appended by spaces. For example, "The U . S . government loaned 100 , 000 dollars . ". I developed 3 different regular expressions to deal with this:
    	- The first regular expression only deals with punctuation marks between numbers, such as "100 , 000" or "134 . 2". I removed the spaces between surrounding such punctuation marks if they are surrounded by numbers, so they become "100,000" or "134.2" respectively. This is done so that the actual number can be preserved.
    	- The second regular expressions deals with all punctuation marks that do not occur between numbers. These punctuation marks are simply removed. So "The U . S . government!" becomes "The US government".
    	- The third regular expressions simply replaces multiple whitespaces between words with single spaces. "The US   government" becomes "The US government".
	- Finally, leading and trailing whitespaces were removed through the `strip()` command.
- Because of this rather extensive text preprocessing, this step takes the most time while running the `gendoc.py` script. It is also because of this kind of text preprocessing that my results might differ a bit from others, because I definitely noticed a changed in the averaged similarity results generated by `simdoc.py` after applying this text preprocessing.

## File naming convention
The output files are as follows:

- `full_output_no_tranf.csv` - term-document matrix with no vocabulary restriction and no other transformations
- `500_basedims_no_transf.csv` - term-document matrix with a vocabulary restriction of 500 dimensions and no other transformations
- `full_output_tfidf.csv` - term-document matrix with no vocabulary restriction, but with tf-idf applied
- `500_basedims_tfidf.csv` - term-document matrix with a vocabulary restriction of 500 dimensions and tf-idf applied
- `full_output_svd_100.csv` - term-document matrix with no vocabulary restriction, with truncated SVD applied to 100 dimensions
- `full_output_svd_1000.csv` - term-document matrix with no vocabulary restriction, with truncated SVD applied to 1000 dimensions
- `full_output_tfidf_svd_100.csv` - term-document matrix with no vocabulary restriction, but with tf-idf applied and truncated SVD applied to 100 dimensions
- `full_output_tfidf_svd_1000.csv` - term-document matrix with no vocabulary restriction, but with tf-idf applied and truncated SVD applied to 1000 dimensions.

## Results and discussion

### Vocabulary restriction.
I chose the size of 500 dimensions for the vocabulary restriction for the output files, because here we are getting the 500 most relevant words over the entire corpus. I wanted to see the difference in similarity by choosing a small amount of the most common words in the corpus, as opposed to using all words in the entire corpus, many of which would have insignificantly small counts, I presume. I also wanted to see the difference in processing time by choosing a much smaller size for the vocabulary.

### Result table

|                            | Crude - Crude | Crude - Grain | Grain - Crude | Grain - Grain |
|----------------------------|---------------|---------------|---------------|---------------|
| full_output_no_tranf       | 0.164885      | 0.079828      | 0.079828      | 0.133035      |
| 500_basedims_no_transf     | 0.245505      | 0.120084      | 0.120084      | 0.190205      |
| full_output_tfidf          | 0.053667      | 0.021270      | 0.021270      | 0.050264      |
| 500_basedims_tfidf         | 0.122172      | 0.050043      | 0.050043      | 0.108147      |
| full_output_svd_100        | 0.305617      | 0.151053      | 0.151053      | 0.256596      |
| full_output_svd_1000       | 0.166038      | 0.080353      | 0.080353      | 0.134683      |
| full_output_tfidf_svd_100  | 0.149384      | 0.061327      | 0.061327      | 0.129242      |
| full_output_tfidf_svd_1000 | 0.054626      | 0.021595      | 0.021595      | 0.051214      |

### The hypothesis in your own words
In my view, this assignment was an attempt to experiment with the various phases involved when working with text data/documents. I think it's a good idea we did not directly move to something like classification or clustering, but first worked with the core methods of representing documents as features outselves, and then applying a similarity measure on it. I also think that this assignment was open-ended enough to encourage self experimentation wherever curiosity arose, such as text preprocessing, vocabulary size, number of dimensions for SVD etc. This gives a pretty detailed insight into the workings of numerical representation of documents and the possibilities that it gives rise to.

### Discussion of trends in results in light of the hypothesis
I have noticed the following trends in the results:
- Limited vocabulary size tends to increase the similarity scores. This makes sense, since the smaller the vocabulary, the less mismatching words there will be in documents
- Reduction of dimensions via SVD tends to increase the similarity scores. This could be attributed to the same reason as above, or because with SVD, the more important features are being considered and a lot of the less useful features adversely affecting the similarity score are removed
- Applying TF-IDF to the word counts tends to decrease the similarity scores. TF-IDF downweights the scores of those terms that are common in the corpus, and gives higher scores to less occurring words in the corpus. This can indicate that the more unique terms in the corpus are specific to the topics, and therefore when these terms are highlighted by being given higher scores, the similarity score goes down. This can also mean that these terms would be a good feature for building a classifier for this dataset
- Initially, the text preprocessing I was applying on the text was removing all punctuation marks, and converting the text to lowercase. However after analyzing the documents, I updated the text preprocessing (as described in detail under "Additional Instructions"), and this decreased the similarity scores. It means that text preprocessing, if done right, can have a major impact on document similarity
- Perhaps I should also have experimented with removing numbers altogether as part of the text preprocessing. I wonder what impact that would have had on the results...

## Bonus answers
##### A short paragraph on what you think the flaws in this experiment might be.
At some points through the development of this experiment, I felt like I was shooting in the dark. For example, if I was getting a certain similarity score between the articles of the two topics, I was not sure whether this is the score that I'm supposed to get, or how I can check or validate (perhaps visually) how or why this similarity score is being generated. Perhaps the latter part is something we were supposed to do on our own, but I feel it should have been encouraged more. 
More to this point, I also feel that perhaps the assignment could perhaps have been branded as "these are two topics, with fairly dissimilar articles so expect low cosine similarities", and see how the similarity scores can vary by applying different feature extraction techniques, or different dimensionality reduction techniques. Just an idea!

##### A short paragraph on how to achieve the same goals in a better wayâ€”for example, other measures we could apply, a better experimental technique, etc.
Instead of playing directly or solely with cosine similarity, perhaps a measure like clustering or topic modeling would have been better-suited to an experiment like this (or at the very least, an interesting alternative!). I would especially like to see what results we would get by applying topic modeling on these articles and see if we can distinguish articles of one topic from the other.